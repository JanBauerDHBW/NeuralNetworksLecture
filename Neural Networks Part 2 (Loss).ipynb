{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# III. Loss (function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss\n",
    "\n",
    "#### Aim:\n",
    "-  Predicted scores should be consistent with training data\n",
    "\n",
    "#### Intuition:\n",
    "-  Loss is high when doing a poor job\n",
    "-  Loss is low when doing a good job\n",
    "\n",
    "$\\leadsto$ interested in distance between prediction and what is in fact true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss\n",
    "\n",
    "Sample average over the data loss using a loss function:\n",
    "$$ L(W) \\equiv L \\equiv \\hat{\\text{E}} L_d \\equiv  \\frac{1}{D} \\sum\\limits_{d=1}^D L_d (y_{[d]},X,W) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss Functions\n",
    "\n",
    "Distance? Just use a norm!\n",
    "\n",
    "- $\\mathcal{L}_1$ norm\n",
    "$$L_d = \\left|\\left|  y_{[d]} - y_{[d]}^\\ast \\right|\\right|_1 \\equiv \\sum\\limits_{k}\\left|  y_{[d]}^{(k)} - y_{[d]}^{\\ast(k)} \\right| $$\n",
    "\n",
    "\n",
    "- $\\mathcal{L}_2$ norm\n",
    "$$L_d = \\left|\\left|  y_{[d]} - y_{[d]}^\\ast \\right|\\right|_2^2 \\equiv \\sum\\limits_{k}\\left(  y_{[d]}^{(k)} - y_{[d]}^{\\ast(k)} \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Foreshadowing:\n",
    "\n",
    "   - Why is squaring the norm possible?\n",
    "   \n",
    "   \n",
    "   - Why do we even square?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Foreshadowing:\n",
    "\n",
    "   - Why is squaring the norm possible? $\\rightarrow$ Squaring is a monotone operation\n",
    "   \n",
    "   \n",
    "   - Why do we even square? $\\rightarrow$ Gradient becomes much simpler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss Functions\n",
    "\n",
    "- Multiclass Support Vector Machine loss (**SVM loss**)\n",
    "$$L_d \\equiv \\sum\\limits_{k\\neq k^\\ast} \\max\\left(0,y_{[d]}^{(k)} - y_{[d]}^{(k^\\ast)} + \\Delta\\right)$$\n",
    "    - we want the model to perform better than by a margin of $\\Delta$\n",
    "    - in pratice: $\\Delta = 1$ (will be clear in Part 4 - Regularization)\n",
    "    - squared loss might perform better, i.e. $\\sum\\max(0,\\cdot)^2$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise Time\n",
    "$$L_d \\equiv \\sum\\limits_{k\\neq k^\\ast} \\max\\left(0,y_{[d]}^{(k)} - y_{[d]}^{(k^\\ast)} + \\Delta\\right)$$\n",
    "- Example: $y=(12, 8, 13, 5)'$, $\\Delta = 2$, $k^\\ast = 1$\n",
    "- Example: $y=(12, 9, 10, 2)'$, $\\Delta = 4$, $k^\\ast = 1$\n",
    "- Example: $y=(12, 8, 13, 5)'$, $\\Delta = 2$, $k^\\ast = 1$ with squared loss\n",
    "- Example: $y=(12, 9, 10, 2)'$, $\\Delta = 4$, $k^\\ast = 1$ with squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss Functions\n",
    "\n",
    "- **Cross-Entropy loss**\n",
    "\n",
    "$$L_d \\equiv - \\log \\dfrac{\\exp\\phi( y_{[d]}^{(k^\\ast)} )}{\\sum\\limits_k \\exp y_{[d]}^{(k)} } = - \\phi( y_{[d]}^{(k^\\ast)} ) + \\log \\sum\\limits_k \\exp y_{[d]}^{(k)} $$\n",
    "\n",
    "    - natural counterpart to the softmax activation function\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cross-Entropy Intuition\n",
    "\n",
    "-  measures the difference between the \"true\" probability distribution $p$ and its estimated counterpart $q$\n",
    "-  discrete case: $H(p,q) = - \\sum\\limits_x p(x) \\log q(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cross-Entropy Intuition\n",
    "\n",
    "-  loss: Set $q(y_{[d]}^{(\\hat{k})}) = \\dfrac{\\exp\\phi( y_{[d]}^{(\\hat{k})} )}{\\sum\\limits_k \\exp y_{[d]}^{(k)} }$ and $p(y_{[d]}^{(k)})$ s.t. $p(y_{[d]}^{(k^\\ast)})=1$ and $p(y_{[d]}^{(k)})=0$ for $k\\neq k^\\ast$. I.e. consider $p(x)$ a distribution with all its mass on the correct class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cross-Entropy Intuition\n",
    "\n",
    "Then\n",
    "\n",
    "$$ H(p,q) = - \\sum\\limits_k p(y_{[d]}^{(k)}) \\log q(y_{[d]}^{(k)}) = - \\sum\\limits_{k\\neq k^\\ast} \\underbrace{p(y_{[d]}^{(k)})}_{=0} \\log q(y_{[d]}^{(k)}) - p(y_{[d]}^{(k^\\ast)}) \\log q(y_{[d]}^{(k^\\ast)}) \\equiv L_d $$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
